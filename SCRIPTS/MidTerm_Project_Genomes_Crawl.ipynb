{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import urllib\n",
    "import urllib.request\n",
    "import shutil\n",
    "import os.path\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Genes_Genomes_Genetics(from_year):\n",
    "    \n",
    "    # My choice of Jornal for the project\n",
    "    main_url = 'https://www.g3journal.org'\n",
    "    \n",
    "    # The List that will be a list of dictionaries and contain all the extracted data \n",
    "    result = []\n",
    "    \n",
    "    # Specifying the Header columns that we want in the result set\n",
    "    fieldnames = ['ID', 'AUTHORS', 'PUBLISH DATE', 'TITLE', 'ABSTRACT', 'FULL PAPER','AFFILIATIONS', 'CORRESPONDENCE AUTHOR', 'KEYWORDS', 'CORRESPONDENCE AUTHOR\\'S E-MAIL']\n",
    "    \n",
    "    # Since we know the Journal started from 2011, this will be used to create the URL for Journal by year\n",
    "    start_year = from_year - 2010\n",
    "    end_year = date.today().year - 2010 + 1\n",
    "\n",
    "    for year in range(start_year,end_year):\n",
    "    # To control for how many years we want to extract the articles\n",
    "    #for year in range(start_year,start_year+2):\n",
    "\n",
    "\n",
    "\n",
    "        # Creating YEAR Folder and path to store html pages\n",
    "        my_folder_year = os.path.join('C:\\\\Users\\\\neha\\\\NJIT\\\\R\\\\HTML\\\\', str(year+2010))\n",
    "        if not os.path.exists(my_folder_year):\n",
    "            os.makedirs(my_folder_year)\n",
    "\n",
    "\n",
    "        # All months in the year or can control the months needed\n",
    "        #for i in range(12,13):        \n",
    "        for i in range(1,13):\n",
    "        \n",
    "            # Creating URL for the Journal by Year and Month\n",
    "            url = main_url+'/content/' + str(year) + '/' +str(i)\n",
    "            req=requests.get(url)\n",
    "            soup = BeautifulSoup(req.text, \"lxml\")\n",
    "            \n",
    "            # Getting all Article Links for a Year and Month\n",
    "            article_links = soup.find_all('div',{'class':'highwire-cite highwire-cite-highwire-article highwire-citation-genetics-list-complete clearfix'})\n",
    "\n",
    "\n",
    "            # Looping through each Article for a month in a year\n",
    "            for idx,article in enumerate(article_links):\n",
    "                total_links = len(article_links)\n",
    "\n",
    "                art_dict = {}\n",
    "\n",
    "                # To Control if we do not want to get all the links from the Page\n",
    "                #if idx in range(0,2):\n",
    "                if idx in range(0,total_links):\n",
    "\n",
    "\n",
    "                    art_dict['ID']=idx+1\n",
    "\n",
    "                    # Getting all Authors\n",
    "                    if article.find_all('span',{'class':'highwire-citation-author'}) is not None:\n",
    "                        other_authors = article.find_all('span',{'class':'highwire-citation-author'})\n",
    "                        for auth_id,auth in enumerate(other_authors):\n",
    "                            art_dict.setdefault('AUTHORS',[]).append(auth.text)\n",
    "\n",
    "\n",
    "                    # URL Link\n",
    "                    art_url = article.find('a',{'class':'highwire-cite-linked-title'}).get('href')\n",
    "                    if re.findall('\\Ahttp',art_url)==[]:\n",
    "                        art_url=main_url+art_url\n",
    "\n",
    "\n",
    "                    # Creating Month Folder and path to store html pages\n",
    "                    my_folder_month = os.path.join(my_folder_year, str(i))\n",
    "                    if not os.path.exists(my_folder_month):\n",
    "                        os.makedirs(my_folder_month)\n",
    "\n",
    "\n",
    "                    # Actual html page is in f variable below\n",
    "                    if article.find('span',{'class':'highwire-cite-metadata-doi highwire-cite-metadata'}) is not None:\n",
    "                        htm = article.find('span',{'class':'highwire-cite-metadata-doi highwire-cite-metadata'}).text\n",
    "                        head , sep, art_htm = htm.partition('https://')\n",
    "                        x = re.sub(r'/', '.', art_htm) + \".html\"\n",
    "                        f = urllib.request.urlretrieve(art_url,x)\n",
    "                        shutil.move('C:\\\\Users\\\\neha\\\\NJIT\\\\R\\\\'+x, os.path.join(my_folder_month, x))\n",
    "\n",
    "\n",
    "                    # TITLE of the Article\n",
    "                    art_text = article.find('a',{'class':'highwire-cite-linked-title'}).text\n",
    "\n",
    "\n",
    "\n",
    "                    # Content of Link\n",
    "                    req_art =  requests.get(art_url)\n",
    "                    soup_art = BeautifulSoup(req_art.text, \"lxml\")\n",
    "\n",
    "                    # Full text\n",
    "                    if soup_art.find(\"div\",{'class':'article fulltext-view'}) is not None:\n",
    "                        art_content = soup_art.find(\"div\",{'class':'article fulltext-view'})\n",
    "                        if art_content.find_all('p') is not None:\n",
    "                            p = art_content.find_all('p')\n",
    "                            content = ' '.join(item .text for item in p)\n",
    "                            art_content = content.encode('utf8','replace')\n",
    "\n",
    "\n",
    "                    # ABSTRACT\n",
    "                    abs_content = soup_art.find(\"div\",{'class':'section abstract'})\n",
    "                    art_content_abs = soup_art.find(\"div\",{'class':'article fulltext-view'})\n",
    "                    abs_content_first = ''\n",
    "                    abs_content_other = ''\n",
    "                    if abs_content is not None:\n",
    "                        q = abs_content.find_all('p')\n",
    "                        content_first = ' '.join(item .text for item in q)\n",
    "                        abs_content_first = content_first.encode('utf8','replace')\n",
    "\n",
    "                        p = art_content_abs.find_all('p',recursive=False)\n",
    "                        content_other = ' '.join(item .text for item in p)\n",
    "                        abs_content_other = content_other.encode('utf8','replace')\n",
    "\n",
    "\n",
    "\n",
    "                    # KEYWORDS\n",
    "                    if soup_art.find_all(\"li\",{'class':'kwd'}) is not None:\n",
    "                        art_keywords = soup_art.find_all(\"li\",{'class':'kwd'})\n",
    "                        for word in art_keywords:\n",
    "                            art_dict.setdefault('KEYWORDS',[]).append(word.text)\n",
    "\n",
    "                    # Going to new url now\n",
    "                    new_url=art_url+'.article-info'\n",
    "                    req_info =  requests.get(new_url)\n",
    "                    soup_info = BeautifulSoup(req_info.text, \"lxml\")\n",
    "\n",
    "                    art_info = soup_info.find('div',{'id':'mini-panel-jnl_genetics_art_info'})\n",
    "                    auth_info = soup_info.find('div',{'id':'content-block-markup'})\n",
    "\n",
    "\n",
    "                    # Published Date \n",
    "                    if art_info.find('li',{'class':'published'}) is not None:\n",
    "                        art_pub = art_info.find('li',{'class':'published'}).text\n",
    "                        head , sep, art_pub = art_pub.partition('online ')\n",
    "\n",
    "\n",
    "                    # EMail\n",
    "                    if auth_info.find_all('span',{'class':'em-link'}) is not None:\n",
    "                        auth_emails = auth_info.find_all('span',{'class':'em-link'})\n",
    "                        for email in auth_emails:\n",
    "                            email_text = re.sub('{at}','@',email.text)\n",
    "                            art_dict.setdefault('CORRESPONDENCE AUTHOR\\'S E-MAIL',[]).append(email_text)\n",
    "\n",
    "                    # Corresponding Author\n",
    "                    if auth_info.find_all('li',{'class':'corresp'}) is not None:\n",
    "                        auth_corr = auth_info.find_all('li',{'class':'corresp'})\n",
    "                        for corr in auth_corr:\n",
    "                            corr_text, sep, tail = corr.text.partition('E-mail')\n",
    "                            corr_text = corr_text.lstrip('â†µ1234567890†↵*')\n",
    "                            art_dict.setdefault('CORRESPONDENCE AUTHOR',[]).append(corr_text)\n",
    "\n",
    "\n",
    "                    # Affiliations\n",
    "                    if auth_info.find_all('li',{'class':'aff'}) is not None:\n",
    "                        auth_affilitiations = auth_info.find_all('li',{'class':'aff'})\n",
    "                        for affi in auth_affilitiations:\n",
    "                            art_dict.setdefault('AFFILIATIONS',[]).append(affi.text.lstrip('â†µ1234567890†↵*'))\n",
    "\n",
    "                    #wrap extracted data in dictionary \n",
    "                    art_dict['PUBLISH DATE'] = art_pub\n",
    "                    art_dict['TITLE'] = art_text\n",
    "                    art_dict['ABSTRACT'] = abs_content_first+abs_content_other\n",
    "                    art_dict['FULL PAPER'] = art_content\n",
    "                    \n",
    "                    \n",
    "                    result.append(art_dict)\n",
    "    \n",
    "    # Writing all the information into a csv file with all the headers defined right at the start\n",
    "    with open('G3_GENOMES.csv', 'w', encoding='utf8', newline='') as output_file:\n",
    "        fc = csv.DictWriter(output_file, \n",
    "                            fieldnames,\n",
    "\n",
    "                           )\n",
    "        fc.writeheader()\n",
    "        fc.writerows(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the Function by passing in Year starting which the Articles will be extracted upto current year(2020)\n",
    "Genes_Genomes_Genetics(2020)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
